From 0a51ff080c3912fc522b268645c9e4df7b39254d Mon Sep 17 00:00:00 2001
From: Peter Jung <admin@ptr1337.dev>
Date: Mon, 31 Oct 2022 17:51:48 +0100
Subject: [PATCH] Revert "sched/fair: Improve scan efficiency of SIS"

This reverts commit 1bff83e8577221017277a3478ec3fc9144740e6e.
---
 include/linux/sched/topology.h |  15 ----
 kernel/sched/fair.c            | 122 ++++-----------------------------
 kernel/sched/features.h        |   7 --
 kernel/sched/sched.h           |   3 -
 kernel/sched/topology.c        |   8 +--
 5 files changed, 14 insertions(+), 141 deletions(-)

diff --git a/include/linux/sched/topology.h b/include/linux/sched/topology.h
index ac2162f33adaf..816df6cc444e1 100644
--- a/include/linux/sched/topology.h
+++ b/include/linux/sched/topology.h
@@ -82,16 +82,6 @@ struct sched_domain_shared {
 	atomic_t	nr_busy_cpus;
 	int		has_idle_cores;
 	int		nr_idle_scan;
-
-	/*
-	 * Used by sched feature SIS_CORE to record idle cpus at core
-	 * granule to improve efficiency of SIS domain scan.
-	 *
-	 * NOTE: this field is variable length. (Allocated dynamically
-	 * by attaching extra space to the end of the structure,
-	 * depending on how many CPUs the kernel has booted up with)
-	 */
-	unsigned long	icpus[];
 };
 
 struct sched_domain {
@@ -177,11 +167,6 @@ static inline struct cpumask *sched_domain_span(struct sched_domain *sd)
 	return to_cpumask(sd->span);
 }
 
-static inline struct cpumask *sched_domain_icpus(struct sched_domain *sd)
-{
-	return to_cpumask(sd->shared->icpus);
-}
-
 extern void partition_sched_domains_locked(int ndoms_new,
 					   cpumask_var_t doms_new[],
 					   struct sched_domain_attr *dattr_new);
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 79a82e57e5d57..947f397cd4263 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -6735,43 +6735,6 @@ static inline bool test_idle_cores(int cpu)
 	return false;
 }
 
-/*
- * To honor the rule of CORE granule update, set this cpu to the LLC idle
- * cpumask only if there is no cpu of this core showed up in the cpumask.
- */
-static void update_idle_cpu(int cpu)
-{
-	struct sched_domain_shared *sds;
-
-	if (!sched_feat(SIS_CORE))
-		return;
-
-	sds = rcu_dereference(per_cpu(sd_llc_shared, cpu));
-	if (sds) {
-		struct cpumask *icpus = to_cpumask(sds->icpus);
-
-		/*
-		 * This is racy against clearing in select_idle_cpu(),
-		 * and can lead to idle cpus miss the chance to be set to
-		 * the idle cpumask, thus the idle cpus are temporarily
-		 * out of reach in SIS domain scan. But it should be rare
-		 * and we still have ILB to kick them working.
-		 */
-		if (!cpumask_intersects(cpu_smt_mask(cpu), icpus))
-			cpumask_set_cpu(cpu, icpus);
-	}
-}
-
-static inline bool should_scan_sibling(int cpu)
-{
-	return cmpxchg(&cpu_rq(cpu)->sis_scan_sibling, 1, 0);
-}
-
-static inline void set_scan_sibling(int cpu)
-{
-	WRITE_ONCE(cpu_rq(cpu)->sis_scan_sibling, 1);
-}
-
 /*
  * Scans the local SMT mask to see if the entire core is idle, and records this
  * information in sd_llc_shared->has_idle_cores.
@@ -6784,11 +6747,7 @@ void __update_idle_core(struct rq *rq)
 	int core = cpu_of(rq);
 	int cpu;
 
-	if (rq->ttwu_pending)
-		return;
-
 	rcu_read_lock();
-	update_idle_cpu(core);
 	if (test_idle_cores(core))
 		goto unlock;
 
@@ -6834,33 +6793,24 @@ static int select_idle_core(struct task_struct *p, int core, struct cpumask *cpu
 	if (idle)
 		return core;
 
-	/*
-	 * It is unlikely that more than one cpu of a core show up
-	 * in the @cpus if SIS_CORE enabled.
-	 */
-	if (!sched_feat(SIS_CORE))
-		cpumask_andnot(cpus, cpus, cpu_smt_mask(core));
-
+	cpumask_andnot(cpus, cpus, cpu_smt_mask(core));
 	return -1;
 }
 
 /*
  * Scan the local SMT mask for idle CPUs.
  */
-static int select_idle_smt(struct task_struct *p, int core, struct cpumask *cpus, int exclude)
+static int select_idle_smt(struct task_struct *p, int target)
 {
 	int cpu;
 
-	for_each_cpu_and(cpu, cpu_smt_mask(core), p->cpus_ptr) {
-		if (exclude && cpu == core)
+	for_each_cpu_and(cpu, cpu_smt_mask(target), p->cpus_ptr) {
+		if (cpu == target)
 			continue;
 		if (available_idle_cpu(cpu) || sched_idle_cpu(cpu))
 			return cpu;
 	}
 
-	if (cpus)
-		cpumask_clear_cpu(core, cpus);
-
 	return -1;
 }
 
@@ -6875,21 +6825,12 @@ static inline bool test_idle_cores(int cpu)
 	return false;
 }
 
-static inline bool should_scan_sibling(int cpu)
-{
-	return false;
-}
-
-static inline void set_scan_sibling(int cpu)
-{
-}
-
 static inline int select_idle_core(struct task_struct *p, int core, struct cpumask *cpus, int *idle_cpu)
 {
 	return __select_idle_cpu(core, p);
 }
 
-static inline int select_idle_smt(struct task_struct *p, int core, struct cpumask *cpus, int exclude)
+static inline int select_idle_smt(struct task_struct *p, int target)
 {
 	return -1;
 }
@@ -6903,15 +6844,16 @@ static inline int select_idle_smt(struct task_struct *p, int core, struct cpumas
  */
 static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool has_idle_core, int target)
 {
-	struct cpumask *cpus = this_cpu_cpumask_var_ptr(select_rq_mask), *icpus = NULL;
+	struct cpumask *cpus = this_cpu_cpumask_var_ptr(select_rq_mask);
 	int i, cpu, idle_cpu = -1, nr = INT_MAX;
 	struct sched_domain_shared *sd_share;
 	struct rq *this_rq = this_rq();
 	int this = smp_processor_id();
 	struct sched_domain *this_sd = NULL;
-	bool scan_sibling = false;
 	u64 time = 0;
 
+	cpumask_and(cpus, sched_domain_span(sd), p->cpus_ptr);
+
 	if (sched_feat(SIS_PROP) && !has_idle_core) {
 		u64 avg_cost, avg_idle, span_avg;
 		unsigned long now = jiffies;
@@ -6944,7 +6886,7 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool
 		time = cpu_clock(this);
 	}
 
-	if (sched_feat(SIS_UTIL) && !has_idle_core) {
+	if (sched_feat(SIS_UTIL)) {
 		sd_share = rcu_dereference(per_cpu(sd_llc_shared, target));
 		if (sd_share) {
 			/* because !--nr is the condition to stop scan */
@@ -6955,31 +6897,15 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool
 		}
 	}
 
-	if (sched_feat(SIS_CORE) && sched_smt_active()) {
-		/*
-		 * Due to the nature of idle core scanning, has_idle_core
-		 * hint should also consume the scan_sibling flag even
-		 * though it doesn't use the flag when scanning.
-		 */
-		scan_sibling = should_scan_sibling(target);
-		icpus = sched_domain_icpus(sd);
-	}
-
-	cpumask_and(cpus, icpus ? icpus : sched_domain_span(sd), p->cpus_ptr);
-
 	for_each_cpu_wrap(cpu, cpus, target + 1) {
-		if (!--nr)
-			break;
-
 		if (has_idle_core) {
 			i = select_idle_core(p, cpu, cpus, &idle_cpu);
 			if ((unsigned int)i < nr_cpumask_bits)
 				return i;
-		} else if (scan_sibling) {
-			idle_cpu = select_idle_smt(p, cpu, icpus, 0);
-			if ((unsigned int)idle_cpu < nr_cpumask_bits)
-				break;
+
 		} else {
+			if (!--nr)
+				return -1;
 			idle_cpu = __select_idle_cpu(cpu, p);
 			if ((unsigned int)idle_cpu < nr_cpumask_bits)
 				break;
@@ -6989,28 +6915,6 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool
 	if (has_idle_core)
 		set_idle_cores(target, false);
 
-	if (icpus && idle_cpu == -1) {
-		if (nr > 0 && (has_idle_core || scan_sibling)) {
-			/*
-			 * Reset the idle cpu mask if a full scan fails,
-			 * but ignore the !has_idle_core case which should
-			 * have already been fixed during scan.
-			 */
-			if (has_idle_core)
-				cpumask_clear(icpus);
-		} else {
-			/*
-			 * As for partial scan failures, it will probably
-			 * fail again next time scanning from the same cpu.
-			 * Due to the SIS_CORE rule of CORE granule update,
-			 * some idle cpus can be missed in the mask. So it
-			 * would be reasonable to scan SMT siblings as well
-			 * if the scan is fail-prone.
-			 */
-			set_scan_sibling(target);
-		}
-	}
-
 	if (sched_feat(SIS_PROP) && this_sd && !has_idle_core) {
 		time = cpu_clock(this) - time;
 
@@ -7167,7 +7071,7 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 		has_idle_core = test_idle_cores(target);
 
 		if (!has_idle_core && cpus_share_cache(prev, target)) {
-			i = select_idle_smt(p, prev, NULL, 1);
+			i = select_idle_smt(p, prev);
 			if ((unsigned int)i < nr_cpumask_bits)
 				return i;
 		}
diff --git a/kernel/sched/features.h b/kernel/sched/features.h
index ab0103957930c..01f3393c38d32 100644
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -75,13 +75,6 @@ SCHED_FEAT(TTWU_QUEUE, true)
 SCHED_FEAT(SIS_PROP, false)
 SCHED_FEAT(SIS_UTIL, true)
 
-/*
- * Record idle cpus at core granule for each LLC to improve efficiency of
- * SIS domain scan. Combine with the above features of limiting scan depth
- * to better deal with the scalability issue.
- */
-SCHED_FEAT(SIS_CORE, true)
-
 /*
  * Issue a WARN when we do multiple update_rq_clock() calls
  * in a single rq->lock section. Default disabled because the
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c68a3821dcf02..2e9ae4d54c202 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1011,9 +1011,6 @@ struct rq {
 
 #ifdef CONFIG_SMP
 	unsigned int		ttwu_pending;
-#ifdef CONFIG_SCHED_SMT
-	int			sis_scan_sibling;
-#endif
 #endif
 	u64			nr_switches;
 
diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index a2bb0091c10d7..8739c2a5a54ea 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -1641,12 +1641,6 @@ sd_init(struct sched_domain_topology_level *tl,
 		sd->shared = *per_cpu_ptr(sdd->sds, sd_id);
 		atomic_inc(&sd->shared->ref);
 		atomic_set(&sd->shared->nr_busy_cpus, sd_weight);
-
-		/*
-		 * This will temporarily break the rule of CORE granule,
-		 * but will be fixed after SIS scan failures.
-		 */
-		cpumask_copy(sched_domain_icpus(sd), sd_span);
 	}
 
 	sd->private = sdd;
@@ -2112,7 +2106,7 @@ static int __sdt_alloc(const struct cpumask *cpu_map)
 
 			*per_cpu_ptr(sdd->sd, j) = sd;
 
-			sds = kzalloc_node(sizeof(struct sched_domain_shared) + cpumask_size(),
+			sds = kzalloc_node(sizeof(struct sched_domain_shared),
 					GFP_KERNEL, cpu_to_node(j));
 			if (!sds)
 				return -ENOMEM;
-- 
2.38.1.381.gc03801e19c

From ce3c8daecd895cbf1a3a557cb3abe501154e4e99 Mon Sep 17 00:00:00 2001
From: Peter Jung <admin@ptr1337.dev>
Date: Mon, 31 Oct 2022 18:15:43 +0100
Subject: [PATCH] Revert "sched"

This reverts commit b6cc509cccc3fc0db1d1f5b8b2d67f8fb848d750.
---
 kernel/sched/core.c     | 239 ++++++++++++-------------------
 kernel/sched/deadline.c |   7 +-
 kernel/sched/fair.c     | 303 ++++++----------------------------------
 kernel/sched/psi.c      |  23 +--
 kernel/sched/sched.h    |  92 +-----------
 5 files changed, 141 insertions(+), 523 deletions(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index b26e90e77bec8..2724dcaabe8e7 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -1403,7 +1403,7 @@ static inline void uclamp_idle_reset(struct rq *rq, enum uclamp_id clamp_id,
 	if (!(rq->uclamp_flags & UCLAMP_FLAG_IDLE))
 		return;
 
-	uclamp_rq_set(rq, clamp_id, clamp_value);
+	WRITE_ONCE(rq->uclamp[clamp_id].value, clamp_value);
 }
 
 static inline
@@ -1554,8 +1554,8 @@ static inline void uclamp_rq_inc_id(struct rq *rq, struct task_struct *p,
 	if (bucket->tasks == 1 || uc_se->value > bucket->value)
 		bucket->value = uc_se->value;
 
-	if (uc_se->value > uclamp_rq_get(rq, clamp_id))
-		uclamp_rq_set(rq, clamp_id, uc_se->value);
+	if (uc_se->value > READ_ONCE(uc_rq->value))
+		WRITE_ONCE(uc_rq->value, uc_se->value);
 }
 
 /*
@@ -1621,7 +1621,7 @@ static inline void uclamp_rq_dec_id(struct rq *rq, struct task_struct *p,
 	if (likely(bucket->tasks))
 		return;
 
-	rq_clamp = uclamp_rq_get(rq, clamp_id);
+	rq_clamp = READ_ONCE(uc_rq->value);
 	/*
 	 * Defensive programming: this should never happen. If it happens,
 	 * e.g. due to future modification, warn and fixup the expected value.
@@ -1629,7 +1629,7 @@ static inline void uclamp_rq_dec_id(struct rq *rq, struct task_struct *p,
 	SCHED_WARN_ON(bucket->value > rq_clamp);
 	if (bucket->value >= rq_clamp) {
 		bkt_clamp = uclamp_rq_max_value(rq, clamp_id, uc_se->value);
-		uclamp_rq_set(rq, clamp_id, bkt_clamp);
+		WRITE_ONCE(uc_rq->value, bkt_clamp);
 	}
 }
 
@@ -2200,18 +2200,14 @@ void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags)
 #ifdef CONFIG_SMP
 
 static void
-__do_set_cpus_allowed(struct task_struct *p, struct affinity_context *ctx);
+__do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask, u32 flags);
 
 static int __set_cpus_allowed_ptr(struct task_struct *p,
-				  struct affinity_context *ctx);
+				  const struct cpumask *new_mask,
+				  u32 flags);
 
 static void migrate_disable_switch(struct rq *rq, struct task_struct *p)
 {
-	struct affinity_context ac = {
-		.new_mask  = cpumask_of(rq->cpu),
-		.flags     = SCA_MIGRATE_DISABLE,
-	};
-
 	if (likely(!p->migration_disabled))
 		return;
 
@@ -2221,7 +2217,7 @@ static void migrate_disable_switch(struct rq *rq, struct task_struct *p)
 	/*
 	 * Violates locking rules! see comment in __do_set_cpus_allowed().
 	 */
-	__do_set_cpus_allowed(p, &ac);
+	__do_set_cpus_allowed(p, cpumask_of(rq->cpu), SCA_MIGRATE_DISABLE);
 }
 
 void migrate_disable(void)
@@ -2243,10 +2239,6 @@ EXPORT_SYMBOL_GPL(migrate_disable);
 void migrate_enable(void)
 {
 	struct task_struct *p = current;
-	struct affinity_context ac = {
-		.new_mask  = &p->cpus_mask,
-		.flags     = SCA_MIGRATE_ENABLE,
-	};
 
 	if (p->migration_disabled > 1) {
 		p->migration_disabled--;
@@ -2262,7 +2254,7 @@ void migrate_enable(void)
 	 */
 	preempt_disable();
 	if (p->cpus_ptr != &p->cpus_mask)
-		__set_cpus_allowed_ptr(p, &ac);
+		__set_cpus_allowed_ptr(p, &p->cpus_mask, SCA_MIGRATE_ENABLE);
 	/*
 	 * Mustn't clear migration_disabled() until cpus_ptr points back at the
 	 * regular cpus_mask, otherwise things that race (eg.
@@ -2542,25 +2534,19 @@ int push_cpu_stop(void *arg)
  * sched_class::set_cpus_allowed must do the below, but is not required to
  * actually call this function.
  */
-void set_cpus_allowed_common(struct task_struct *p, struct affinity_context *ctx)
+void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask, u32 flags)
 {
-	if (ctx->flags & (SCA_MIGRATE_ENABLE | SCA_MIGRATE_DISABLE)) {
-		p->cpus_ptr = ctx->new_mask;
+	if (flags & (SCA_MIGRATE_ENABLE | SCA_MIGRATE_DISABLE)) {
+		p->cpus_ptr = new_mask;
 		return;
 	}
 
-	cpumask_copy(&p->cpus_mask, ctx->new_mask);
-	p->nr_cpus_allowed = cpumask_weight(ctx->new_mask);
-
-	/*
-	 * Swap in a new user_cpus_ptr if SCA_USER flag set
-	 */
-	if (ctx->flags & SCA_USER)
-		swap(p->user_cpus_ptr, ctx->user_mask);
+	cpumask_copy(&p->cpus_mask, new_mask);
+	p->nr_cpus_allowed = cpumask_weight(new_mask);
 }
 
 static void
-__do_set_cpus_allowed(struct task_struct *p, struct affinity_context *ctx)
+__do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask, u32 flags)
 {
 	struct rq *rq = task_rq(p);
 	bool queued, running;
@@ -2577,7 +2563,7 @@ __do_set_cpus_allowed(struct task_struct *p, struct affinity_context *ctx)
 	 *
 	 * XXX do further audits, this smells like something putrid.
 	 */
-	if (ctx->flags & SCA_MIGRATE_DISABLE)
+	if (flags & SCA_MIGRATE_DISABLE)
 		SCHED_WARN_ON(!p->on_cpu);
 	else
 		lockdep_assert_held(&p->pi_lock);
@@ -2596,7 +2582,7 @@ __do_set_cpus_allowed(struct task_struct *p, struct affinity_context *ctx)
 	if (running)
 		put_prev_task(rq, p);
 
-	p->sched_class->set_cpus_allowed(p, ctx);
+	p->sched_class->set_cpus_allowed(p, new_mask, flags);
 
 	if (queued)
 		enqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);
@@ -2604,27 +2590,14 @@ __do_set_cpus_allowed(struct task_struct *p, struct affinity_context *ctx)
 		set_next_task(rq, p);
 }
 
-/*
- * Used for kthread_bind() and select_fallback_rq(), in both cases the user
- * affinity (if any) should be destroyed too.
- */
 void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
 {
-	struct affinity_context ac = {
-		.new_mask  = new_mask,
-		.user_mask = NULL,
-		.flags     = SCA_USER,	/* clear the user requested mask */
-	};
-
-	__do_set_cpus_allowed(p, &ac);
-	kfree(ac.user_mask);
+	__do_set_cpus_allowed(p, new_mask, 0);
 }
 
 int dup_user_cpus_ptr(struct task_struct *dst, struct task_struct *src,
 		      int node)
 {
-	unsigned long flags;
-
 	if (!src->user_cpus_ptr)
 		return 0;
 
@@ -2632,10 +2605,7 @@ int dup_user_cpus_ptr(struct task_struct *dst, struct task_struct *src,
 	if (!dst->user_cpus_ptr)
 		return -ENOMEM;
 
-	/* Use pi_lock to protect content of user_cpus_ptr */
-	raw_spin_lock_irqsave(&src->pi_lock, flags);
 	cpumask_copy(dst->user_cpus_ptr, src->user_cpus_ptr);
-	raw_spin_unlock_irqrestore(&src->pi_lock, flags);
 	return 0;
 }
 
@@ -2731,8 +2701,6 @@ void release_user_cpus_ptr(struct task_struct *p)
  */
 static int affine_move_task(struct rq *rq, struct task_struct *p, struct rq_flags *rf,
 			    int dest_cpu, unsigned int flags)
-	__releases(rq->lock)
-	__releases(p->pi_lock)
 {
 	struct set_affinity_pending my_pending = { }, *pending = NULL;
 	bool stop_pending, complete = false;
@@ -2875,7 +2843,8 @@ static int affine_move_task(struct rq *rq, struct task_struct *p, struct rq_flag
  * Called with both p->pi_lock and rq->lock held; drops both before returning.
  */
 static int __set_cpus_allowed_ptr_locked(struct task_struct *p,
-					 struct affinity_context *ctx,
+					 const struct cpumask *new_mask,
+					 u32 flags,
 					 struct rq *rq,
 					 struct rq_flags *rf)
 	__releases(rq->lock)
@@ -2884,6 +2853,7 @@ static int __set_cpus_allowed_ptr_locked(struct task_struct *p,
 	const struct cpumask *cpu_allowed_mask = task_cpu_possible_mask(p);
 	const struct cpumask *cpu_valid_mask = cpu_active_mask;
 	bool kthread = p->flags & PF_KTHREAD;
+	struct cpumask *user_mask = NULL;
 	unsigned int dest_cpu;
 	int ret = 0;
 
@@ -2903,7 +2873,7 @@ static int __set_cpus_allowed_ptr_locked(struct task_struct *p,
 		cpu_valid_mask = cpu_online_mask;
 	}
 
-	if (!kthread && !cpumask_subset(ctx->new_mask, cpu_allowed_mask)) {
+	if (!kthread && !cpumask_subset(new_mask, cpu_allowed_mask)) {
 		ret = -EINVAL;
 		goto out;
 	}
@@ -2912,18 +2882,18 @@ static int __set_cpus_allowed_ptr_locked(struct task_struct *p,
 	 * Must re-check here, to close a race against __kthread_bind(),
 	 * sched_setaffinity() is not guaranteed to observe the flag.
 	 */
-	if ((ctx->flags & SCA_CHECK) && (p->flags & PF_NO_SETAFFINITY)) {
+	if ((flags & SCA_CHECK) && (p->flags & PF_NO_SETAFFINITY)) {
 		ret = -EINVAL;
 		goto out;
 	}
 
-	if (!(ctx->flags & SCA_MIGRATE_ENABLE)) {
-		if (cpumask_equal(&p->cpus_mask, ctx->new_mask))
+	if (!(flags & SCA_MIGRATE_ENABLE)) {
+		if (cpumask_equal(&p->cpus_mask, new_mask))
 			goto out;
 
 		if (WARN_ON_ONCE(p == current &&
 				 is_migration_disabled(p) &&
-				 !cpumask_test_cpu(task_cpu(p), ctx->new_mask))) {
+				 !cpumask_test_cpu(task_cpu(p), new_mask))) {
 			ret = -EBUSY;
 			goto out;
 		}
@@ -2934,15 +2904,22 @@ static int __set_cpus_allowed_ptr_locked(struct task_struct *p,
 	 * for groups of tasks (ie. cpuset), so that load balancing is not
 	 * immediately required to distribute the tasks within their new mask.
 	 */
-	dest_cpu = cpumask_any_and_distribute(cpu_valid_mask, ctx->new_mask);
+	dest_cpu = cpumask_any_and_distribute(cpu_valid_mask, new_mask);
 	if (dest_cpu >= nr_cpu_ids) {
 		ret = -EINVAL;
 		goto out;
 	}
 
-	__do_set_cpus_allowed(p, ctx);
+	__do_set_cpus_allowed(p, new_mask, flags);
 
-	return affine_move_task(rq, p, rf, dest_cpu, ctx->flags);
+	if (flags & SCA_USER)
+		user_mask = clear_user_cpus_ptr(p);
+
+	ret = affine_move_task(rq, p, rf, dest_cpu, flags);
+
+	kfree(user_mask);
+
+	return ret;
 
 out:
 	task_rq_unlock(rq, p, rf);
@@ -2960,41 +2937,25 @@ static int __set_cpus_allowed_ptr_locked(struct task_struct *p,
  * call is not atomic; no spinlocks may be held.
  */
 static int __set_cpus_allowed_ptr(struct task_struct *p,
-				  struct affinity_context *ctx)
+				  const struct cpumask *new_mask, u32 flags)
 {
 	struct rq_flags rf;
 	struct rq *rq;
 
 	rq = task_rq_lock(p, &rf);
-	/*
-	 * Masking should be skipped if SCA_USER or any of the SCA_MIGRATE_*
-	 * flags are set.
-	 */
-	if (p->user_cpus_ptr &&
-	    !(ctx->flags & (SCA_USER | SCA_MIGRATE_ENABLE | SCA_MIGRATE_DISABLE)) &&
-	    cpumask_and(rq->scratch_mask, ctx->new_mask, p->user_cpus_ptr))
-		ctx->new_mask = rq->scratch_mask;
-
-	return __set_cpus_allowed_ptr_locked(p, ctx, rq, &rf);
+	return __set_cpus_allowed_ptr_locked(p, new_mask, flags, rq, &rf);
 }
 
 int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)
 {
-	struct affinity_context ac = {
-		.new_mask  = new_mask,
-		.flags     = 0,
-	};
-
-	return __set_cpus_allowed_ptr(p, &ac);
+	return __set_cpus_allowed_ptr(p, new_mask, 0);
 }
 EXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);
 
 /*
  * Change a given task's CPU affinity to the intersection of its current
- * affinity mask and @subset_mask, writing the resulting mask to @new_mask.
- * If user_cpus_ptr is defined, use it as the basis for restricting CPU
- * affinity or use cpu_online_mask instead.
- *
+ * affinity mask and @subset_mask, writing the resulting mask to @new_mask
+ * and pointing @p->user_cpus_ptr to a copy of the old mask.
  * If the resulting mask is empty, leave the affinity unchanged and return
  * -EINVAL.
  */
@@ -3002,14 +2963,17 @@ static int restrict_cpus_allowed_ptr(struct task_struct *p,
 				     struct cpumask *new_mask,
 				     const struct cpumask *subset_mask)
 {
-	struct affinity_context ac = {
-		.new_mask  = new_mask,
-		.flags     = 0,
-	};
+	struct cpumask *user_mask = NULL;
 	struct rq_flags rf;
 	struct rq *rq;
 	int err;
 
+	if (!p->user_cpus_ptr) {
+		user_mask = kmalloc(cpumask_size(), GFP_KERNEL);
+		if (!user_mask)
+			return -ENOMEM;
+	}
+
 	rq = task_rq_lock(p, &rf);
 
 	/*
@@ -3022,21 +2986,31 @@ static int restrict_cpus_allowed_ptr(struct task_struct *p,
 		goto err_unlock;
 	}
 
-	if (!cpumask_and(new_mask, task_user_cpus(p), subset_mask)) {
+	if (!cpumask_and(new_mask, &p->cpus_mask, subset_mask)) {
 		err = -EINVAL;
 		goto err_unlock;
 	}
 
-	return __set_cpus_allowed_ptr_locked(p, &ac, rq, &rf);
+	/*
+	 * We're about to butcher the task affinity, so keep track of what
+	 * the user asked for in case we're able to restore it later on.
+	 */
+	if (user_mask) {
+		cpumask_copy(user_mask, p->cpus_ptr);
+		p->user_cpus_ptr = user_mask;
+	}
+
+	return __set_cpus_allowed_ptr_locked(p, new_mask, 0, rq, &rf);
 
 err_unlock:
 	task_rq_unlock(rq, p, &rf);
+	kfree(user_mask);
 	return err;
 }
 
 /*
  * Restrict the CPU affinity of task @p so that it is a subset of
- * task_cpu_possible_mask() and point @p->user_cpus_ptr to a copy of the
+ * task_cpu_possible_mask() and point @p->user_cpu_ptr to a copy of the
  * old affinity mask. If the resulting mask is empty, we warn and walk
  * up the cpuset hierarchy until we find a suitable mask.
  */
@@ -3080,29 +3054,34 @@ void force_compatible_cpus_allowed_ptr(struct task_struct *p)
 }
 
 static int
-__sched_setaffinity(struct task_struct *p, struct affinity_context *ctx);
+__sched_setaffinity(struct task_struct *p, const struct cpumask *mask);
 
 /*
  * Restore the affinity of a task @p which was previously restricted by a
- * call to force_compatible_cpus_allowed_ptr().
+ * call to force_compatible_cpus_allowed_ptr(). This will clear (and free)
+ * @p->user_cpus_ptr.
  *
  * It is the caller's responsibility to serialise this with any calls to
  * force_compatible_cpus_allowed_ptr(@p).
  */
 void relax_compatible_cpus_allowed_ptr(struct task_struct *p)
 {
-	struct affinity_context ac = {
-		.new_mask  = task_user_cpus(p),
-		.flags     = 0,
-	};
-	int ret;
+	struct cpumask *user_mask = p->user_cpus_ptr;
+	unsigned long flags;
 
 	/*
-	 * Try to restore the old affinity mask with __sched_setaffinity().
-	 * Cpuset masking will be done there too.
+	 * Try to restore the old affinity mask. If this fails, then
+	 * we free the mask explicitly to avoid it being inherited across
+	 * a subsequent fork().
 	 */
-	ret = __sched_setaffinity(p, &ac);
-	WARN_ON_ONCE(ret);
+	if (!user_mask || !__sched_setaffinity(p, user_mask))
+		return;
+
+	raw_spin_lock_irqsave(&p->pi_lock, flags);
+	user_mask = clear_user_cpus_ptr(p);
+	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
+
+	kfree(user_mask);
 }
 
 void set_task_cpu(struct task_struct *p, unsigned int new_cpu)
@@ -3580,9 +3559,10 @@ void sched_set_stop_task(int cpu, struct task_struct *stop)
 #else /* CONFIG_SMP */
 
 static inline int __set_cpus_allowed_ptr(struct task_struct *p,
-					 struct affinity_context *ctx)
+					 const struct cpumask *new_mask,
+					 u32 flags)
 {
-	return set_cpus_allowed_ptr(p, ctx->new_mask);
+	return set_cpus_allowed_ptr(p, new_mask);
 }
 
 static inline void migrate_disable_switch(struct rq *rq, struct task_struct *p) { }
@@ -8150,7 +8130,7 @@ int dl_task_check_affinity(struct task_struct *p, const struct cpumask *mask)
 #endif
 
 static int
-__sched_setaffinity(struct task_struct *p, struct affinity_context *ctx)
+__sched_setaffinity(struct task_struct *p, const struct cpumask *mask)
 {
 	int retval;
 	cpumask_var_t cpus_allowed, new_mask;
@@ -8164,16 +8144,13 @@ __sched_setaffinity(struct task_struct *p, struct affinity_context *ctx)
 	}
 
 	cpuset_cpus_allowed(p, cpus_allowed);
-	cpumask_and(new_mask, ctx->new_mask, cpus_allowed);
-
-	ctx->new_mask = new_mask;
-	ctx->flags |= SCA_CHECK;
+	cpumask_and(new_mask, mask, cpus_allowed);
 
 	retval = dl_task_check_affinity(p, new_mask);
 	if (retval)
 		goto out_free_new_mask;
-
-	retval = __set_cpus_allowed_ptr(p, ctx);
+again:
+	retval = __set_cpus_allowed_ptr(p, new_mask, SCA_CHECK | SCA_USER);
 	if (retval)
 		goto out_free_new_mask;
 
@@ -8184,24 +8161,7 @@ __sched_setaffinity(struct task_struct *p, struct affinity_context *ctx)
 		 * Just reset the cpumask to the cpuset's cpus_allowed.
 		 */
 		cpumask_copy(new_mask, cpus_allowed);
-
-		/*
-		 * If SCA_USER is set, a 2nd call to __set_cpus_allowed_ptr()
-		 * will restore the previous user_cpus_ptr value.
-		 *
-		 * In the unlikely event a previous user_cpus_ptr exists,
-		 * we need to further restrict the mask to what is allowed
-		 * by that old user_cpus_ptr.
-		 */
-		if (unlikely((ctx->flags & SCA_USER) && ctx->user_mask)) {
-			bool empty = !cpumask_and(new_mask, new_mask,
-						  ctx->user_mask);
-
-			if (WARN_ON_ONCE(empty))
-				cpumask_copy(new_mask, cpus_allowed);
-		}
-		__set_cpus_allowed_ptr(p, ctx);
-		retval = -EINVAL;
+		goto again;
 	}
 
 out_free_new_mask:
@@ -8213,8 +8173,6 @@ __sched_setaffinity(struct task_struct *p, struct affinity_context *ctx)
 
 long sched_setaffinity(pid_t pid, const struct cpumask *in_mask)
 {
-	struct affinity_context ac;
-	struct cpumask *user_mask;
 	struct task_struct *p;
 	int retval;
 
@@ -8249,21 +8207,7 @@ long sched_setaffinity(pid_t pid, const struct cpumask *in_mask)
 	if (retval)
 		goto out_put_task;
 
-	user_mask = kmalloc(cpumask_size(), GFP_KERNEL);
-	if (!user_mask) {
-		retval = -ENOMEM;
-		goto out_put_task;
-	}
-	cpumask_copy(user_mask, in_mask);
-	ac = (struct affinity_context){
-		.new_mask  = in_mask,
-		.user_mask = user_mask,
-		.flags     = SCA_USER,
-	};
-
-	retval = __sched_setaffinity(p, &ac);
-	kfree(ac.user_mask);
-
+	retval = __sched_setaffinity(p, in_mask);
 out_put_task:
 	put_task_struct(p);
 	return retval;
@@ -9044,12 +8988,6 @@ void show_state_filter(unsigned int state_filter)
  */
 void __init init_idle(struct task_struct *idle, int cpu)
 {
-#ifdef CONFIG_SMP
-	struct affinity_context ac = (struct affinity_context) {
-		.new_mask  = cpumask_of(cpu),
-		.flags     = 0,
-	};
-#endif
 	struct rq *rq = cpu_rq(cpu);
 	unsigned long flags;
 
@@ -9074,7 +9012,7 @@ void __init init_idle(struct task_struct *idle, int cpu)
 	 *
 	 * And since this is boot we can forgo the serialization.
 	 */
-	set_cpus_allowed_common(idle, &ac);
+	set_cpus_allowed_common(idle, cpumask_of(cpu), 0);
 #endif
 	/*
 	 * We're having a chicken and egg problem, even though we are
@@ -9865,7 +9803,6 @@ void __init sched_init(void)
 
 		rq->core_cookie = 0UL;
 #endif
-		zalloc_cpumask_var_node(&rq->scratch_mask, GFP_KERNEL, cpu_to_node(i));
 	}
 
 	set_load_weight(&init_task, false);
diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index 0d97d54276cc8..9ae8f41e3372f 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -2485,7 +2485,8 @@ static void task_woken_dl(struct rq *rq, struct task_struct *p)
 }
 
 static void set_cpus_allowed_dl(struct task_struct *p,
-				struct affinity_context *ctx)
+				const struct cpumask *new_mask,
+				u32 flags)
 {
 	struct root_domain *src_rd;
 	struct rq *rq;
@@ -2500,7 +2501,7 @@ static void set_cpus_allowed_dl(struct task_struct *p,
 	 * update. We already made space for us in the destination
 	 * domain (see cpuset_can_attach()).
 	 */
-	if (!cpumask_intersects(src_rd->span, ctx->new_mask)) {
+	if (!cpumask_intersects(src_rd->span, new_mask)) {
 		struct dl_bw *src_dl_b;
 
 		src_dl_b = dl_bw_of(cpu_of(rq));
@@ -2514,7 +2515,7 @@ static void set_cpus_allowed_dl(struct task_struct *p,
 		raw_spin_unlock(&src_dl_b->lock);
 	}
 
-	set_cpus_allowed_common(p, ctx);
+	set_cpus_allowed_common(p, new_mask, flags);
 }
 
 /* Assumes rq->lock is held */
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 947f397cd4263..3574b93e71af8 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -4409,16 +4409,14 @@ static inline unsigned long task_util_est(struct task_struct *p)
 }
 
 #ifdef CONFIG_UCLAMP_TASK
-static inline unsigned long uclamp_task_util(struct task_struct *p,
-					     unsigned long uclamp_min,
-					     unsigned long uclamp_max)
+static inline unsigned long uclamp_task_util(struct task_struct *p)
 {
-	return clamp(task_util_est(p), uclamp_min, uclamp_max);
+	return clamp(task_util_est(p),
+		     uclamp_eff_value(p, UCLAMP_MIN),
+		     uclamp_eff_value(p, UCLAMP_MAX));
 }
 #else
-static inline unsigned long uclamp_task_util(struct task_struct *p,
-					     unsigned long uclamp_min,
-					     unsigned long uclamp_max)
+static inline unsigned long uclamp_task_util(struct task_struct *p)
 {
 	return task_util_est(p);
 }
@@ -4557,139 +4555,10 @@ static inline void util_est_update(struct cfs_rq *cfs_rq,
 	trace_sched_util_est_se_tp(&p->se);
 }
 
-static inline int util_fits_cpu(unsigned long util,
-				unsigned long uclamp_min,
-				unsigned long uclamp_max,
-				int cpu)
+static inline int task_fits_capacity(struct task_struct *p,
+				     unsigned long capacity)
 {
-	unsigned long capacity_orig, capacity_orig_thermal;
-	unsigned long capacity = capacity_of(cpu);
-	bool fits, uclamp_max_fits;
-
-	/*
-	 * Check if the real util fits without any uclamp boost/cap applied.
-	 */
-	fits = fits_capacity(util, capacity);
-
-	if (!uclamp_is_used())
-		return fits;
-
-	/*
-	 * We must use capacity_orig_of() for comparing against uclamp_min and
-	 * uclamp_max. We only care about capacity pressure (by using
-	 * capacity_of()) for comparing against the real util.
-	 *
-	 * If a task is boosted to 1024 for example, we don't want a tiny
-	 * pressure to skew the check whether it fits a CPU or not.
-	 *
-	 * Similarly if a task is capped to capacity_orig_of(little_cpu), it
-	 * should fit a little cpu even if there's some pressure.
-	 *
-	 * Only exception is for thermal pressure since it has a direct impact
-	 * on available OPP of the system.
-	 *
-	 * We honour it for uclamp_min only as a drop in performance level
-	 * could result in not getting the requested minimum performance level.
-	 *
-	 * For uclamp_max, we can tolerate a drop in performance level as the
-	 * goal is to cap the task. So it's okay if it's getting less.
-	 *
-	 * In case of capacity inversion we should honour the inverted capacity
-	 * for both uclamp_min and uclamp_max all the time.
-	 */
-	capacity_orig = cpu_in_capacity_inversion(cpu);
-	if (capacity_orig) {
-		capacity_orig_thermal = capacity_orig;
-	} else {
-		capacity_orig = capacity_orig_of(cpu);
-		capacity_orig_thermal = capacity_orig - arch_scale_thermal_pressure(cpu);
-	}
-
-	/*
-	 * We want to force a task to fit a cpu as implied by uclamp_max.
-	 * But we do have some corner cases to cater for..
-	 *
-	 *
-	 *                                 C=z
-	 *   |                             ___
-	 *   |                  C=y       |   |
-	 *   |_ _ _ _ _ _ _ _ _ ___ _ _ _ | _ | _ _ _ _ _  uclamp_max
-	 *   |      C=x        |   |      |   |
-	 *   |      ___        |   |      |   |
-	 *   |     |   |       |   |      |   |    (util somewhere in this region)
-	 *   |     |   |       |   |      |   |
-	 *   |     |   |       |   |      |   |
-	 *   +----------------------------------------
-	 *         cpu0        cpu1       cpu2
-	 *
-	 *   In the above example if a task is capped to a specific performance
-	 *   point, y, then when:
-	 *
-	 *   * util = 80% of x then it does not fit on cpu0 and should migrate
-	 *     to cpu1
-	 *   * util = 80% of y then it is forced to fit on cpu1 to honour
-	 *     uclamp_max request.
-	 *
-	 *   which is what we're enforcing here. A task always fits if
-	 *   uclamp_max <= capacity_orig. But when uclamp_max > capacity_orig,
-	 *   the normal upmigration rules should withhold still.
-	 *
-	 *   Only exception is when we are on max capacity, then we need to be
-	 *   careful not to block overutilized state. This is so because:
-	 *
-	 *     1. There's no concept of capping at max_capacity! We can't go
-	 *        beyond this performance level anyway.
-	 *     2. The system is being saturated when we're operating near
-	 *        max capacity, it doesn't make sense to block overutilized.
-	 */
-	uclamp_max_fits = (capacity_orig == SCHED_CAPACITY_SCALE) && (uclamp_max == SCHED_CAPACITY_SCALE);
-	uclamp_max_fits = !uclamp_max_fits && (uclamp_max <= capacity_orig);
-	fits = fits || uclamp_max_fits;
-
-	/*
-	 *
-	 *                                 C=z
-	 *   |                             ___       (region a, capped, util >= uclamp_max)
-	 *   |                  C=y       |   |
-	 *   |_ _ _ _ _ _ _ _ _ ___ _ _ _ | _ | _ _ _ _ _ uclamp_max
-	 *   |      C=x        |   |      |   |
-	 *   |      ___        |   |      |   |      (region b, uclamp_min <= util <= uclamp_max)
-	 *   |_ _ _|_ _|_ _ _ _| _ | _ _ _| _ | _ _ _ _ _ uclamp_min
-	 *   |     |   |       |   |      |   |
-	 *   |     |   |       |   |      |   |      (region c, boosted, util < uclamp_min)
-	 *   +----------------------------------------
-	 *         cpu0        cpu1       cpu2
-	 *
-	 * a) If util > uclamp_max, then we're capped, we don't care about
-	 *    actual fitness value here. We only care if uclamp_max fits
-	 *    capacity without taking margin/pressure into account.
-	 *    See comment above.
-	 *
-	 * b) If uclamp_min <= util <= uclamp_max, then the normal
-	 *    fits_capacity() rules apply. Except we need to ensure that we
-	 *    enforce we remain within uclamp_max, see comment above.
-	 *
-	 * c) If util < uclamp_min, then we are boosted. Same as (b) but we
-	 *    need to take into account the boosted value fits the CPU without
-	 *    taking margin/pressure into account.
-	 *
-	 * Cases (a) and (b) are handled in the 'fits' variable already. We
-	 * just need to consider an extra check for case (c) after ensuring we
-	 * handle the case uclamp_min > uclamp_max.
-	 */
-	uclamp_min = min(uclamp_min, uclamp_max);
-	if (util < uclamp_min && capacity_orig != SCHED_CAPACITY_SCALE)
-		fits = fits && (uclamp_min <= capacity_orig_thermal);
-
-	return fits;
-}
-
-static inline int task_fits_cpu(struct task_struct *p, int cpu)
-{
-	unsigned long uclamp_min = uclamp_eff_value(p, UCLAMP_MIN);
-	unsigned long uclamp_max = uclamp_eff_value(p, UCLAMP_MAX);
-	unsigned long util = task_util_est(p);
-	return util_fits_cpu(util, uclamp_min, uclamp_max, cpu);
+	return fits_capacity(uclamp_task_util(p), capacity);
 }
 
 static inline void update_misfit_status(struct task_struct *p, struct rq *rq)
@@ -4702,7 +4571,7 @@ static inline void update_misfit_status(struct task_struct *p, struct rq *rq)
 		return;
 	}
 
-	if (task_fits_cpu(p, cpu_of(rq))) {
+	if (task_fits_capacity(p, capacity_of(cpu_of(rq)))) {
 		rq->misfit_task_load = 0;
 		return;
 	}
@@ -6140,10 +6009,7 @@ static inline void hrtick_update(struct rq *rq)
 #ifdef CONFIG_SMP
 static inline bool cpu_overutilized(int cpu)
 {
-	unsigned long rq_util_min = uclamp_rq_get(cpu_rq(cpu), UCLAMP_MIN);
-	unsigned long rq_util_max = uclamp_rq_get(cpu_rq(cpu), UCLAMP_MAX);
-
-	return !util_fits_cpu(cpu_util_cfs(cpu), rq_util_min, rq_util_max, cpu);
+	return !fits_capacity(cpu_util_cfs(cpu), capacity_of(cpu));
 }
 
 static inline void update_overutilized_status(struct rq *rq)
@@ -6938,23 +6804,21 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool
 static int
 select_idle_capacity(struct task_struct *p, struct sched_domain *sd, int target)
 {
-	unsigned long task_util, util_min, util_max, best_cap = 0;
+	unsigned long task_util, best_cap = 0;
 	int cpu, best_cpu = -1;
 	struct cpumask *cpus;
 
 	cpus = this_cpu_cpumask_var_ptr(select_rq_mask);
 	cpumask_and(cpus, sched_domain_span(sd), p->cpus_ptr);
 
-	task_util = task_util_est(p);
-	util_min = uclamp_eff_value(p, UCLAMP_MIN);
-	util_max = uclamp_eff_value(p, UCLAMP_MAX);
+	task_util = uclamp_task_util(p);
 
 	for_each_cpu_wrap(cpu, cpus, target) {
 		unsigned long cpu_cap = capacity_of(cpu);
 
 		if (!available_idle_cpu(cpu) && !sched_idle_cpu(cpu))
 			continue;
-		if (util_fits_cpu(task_util, util_min, util_max, cpu))
+		if (fits_capacity(task_util, cpu_cap))
 			return cpu;
 
 		if (cpu_cap > best_cap) {
@@ -6966,13 +6830,10 @@ select_idle_capacity(struct task_struct *p, struct sched_domain *sd, int target)
 	return best_cpu;
 }
 
-static inline bool asym_fits_cpu(unsigned long util,
-				 unsigned long util_min,
-				 unsigned long util_max,
-				 int cpu)
+static inline bool asym_fits_capacity(unsigned long task_util, int cpu)
 {
 	if (sched_asym_cpucap_active())
-		return util_fits_cpu(util, util_min, util_max, cpu);
+		return fits_capacity(task_util, capacity_of(cpu));
 
 	return true;
 }
@@ -6984,7 +6845,7 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 {
 	bool has_idle_core = false;
 	struct sched_domain *sd;
-	unsigned long task_util, util_min, util_max;
+	unsigned long task_util;
 	int i, recent_used_cpu;
 
 	/*
@@ -6993,9 +6854,7 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	 */
 	if (sched_asym_cpucap_active()) {
 		sync_entity_load_avg(&p->se);
-		task_util = task_util_est(p);
-		util_min = uclamp_eff_value(p, UCLAMP_MIN);
-		util_max = uclamp_eff_value(p, UCLAMP_MAX);
+		task_util = uclamp_task_util(p);
 	}
 
 	/*
@@ -7004,7 +6863,7 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	lockdep_assert_irqs_disabled();
 
 	if ((available_idle_cpu(target) || sched_idle_cpu(target)) &&
-	    asym_fits_cpu(task_util, util_min, util_max, target))
+	    asym_fits_capacity(task_util, target))
 		return target;
 
 	/*
@@ -7012,7 +6871,7 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	 */
 	if (prev != target && cpus_share_cache(prev, target) &&
 	    (available_idle_cpu(prev) || sched_idle_cpu(prev)) &&
-	    asym_fits_cpu(task_util, util_min, util_max, prev))
+	    asym_fits_capacity(task_util, prev))
 		return prev;
 
 	/*
@@ -7027,7 +6886,7 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	    in_task() &&
 	    prev == smp_processor_id() &&
 	    this_rq()->nr_running <= 1 &&
-	    asym_fits_cpu(task_util, util_min, util_max, prev)) {
+	    asym_fits_capacity(task_util, prev)) {
 		return prev;
 	}
 
@@ -7039,7 +6898,7 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	    cpus_share_cache(recent_used_cpu, target) &&
 	    (available_idle_cpu(recent_used_cpu) || sched_idle_cpu(recent_used_cpu)) &&
 	    cpumask_test_cpu(p->recent_used_cpu, p->cpus_ptr) &&
-	    asym_fits_cpu(task_util, util_min, util_max, recent_used_cpu)) {
+	    asym_fits_capacity(task_util, recent_used_cpu)) {
 		return recent_used_cpu;
 	}
 
@@ -7335,8 +7194,6 @@ static int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu)
 {
 	struct cpumask *cpus = this_cpu_cpumask_var_ptr(select_rq_mask);
 	unsigned long prev_delta = ULONG_MAX, best_delta = ULONG_MAX;
-	unsigned long p_util_min = uclamp_is_used() ? uclamp_eff_value(p, UCLAMP_MIN) : 0;
-	unsigned long p_util_max = uclamp_is_used() ? uclamp_eff_value(p, UCLAMP_MAX) : 1024;
 	struct root_domain *rd = this_rq()->rd;
 	int cpu, best_energy_cpu, target = -1;
 	struct sched_domain *sd;
@@ -7361,7 +7218,7 @@ static int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu)
 	target = prev_cpu;
 
 	sync_entity_load_avg(&p->se);
-	if (!uclamp_task_util(p, p_util_min, p_util_max))
+	if (!task_util_est(p))
 		goto unlock;
 
 	eenv_task_busy_time(&eenv, p, prev_cpu);
@@ -7369,9 +7226,7 @@ static int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu)
 	for (; pd; pd = pd->next) {
 		unsigned long cpu_cap, cpu_thermal_cap, util;
 		unsigned long cur_delta, max_spare_cap = 0;
-		unsigned long rq_util_min, rq_util_max;
-		unsigned long util_min, util_max;
-		unsigned long prev_spare_cap = 0;
+		bool compute_prev_delta = false;
 		int max_spare_cap_cpu = -1;
 		unsigned long base_energy;
 
@@ -7407,45 +7262,26 @@ static int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu)
 			 * much capacity we can get out of the CPU; this is
 			 * aligned with sched_cpu_util().
 			 */
-			if (uclamp_is_used()) {
-				if (uclamp_rq_is_idle(cpu_rq(cpu))) {
-					util_min = p_util_min;
-					util_max = p_util_max;
-				} else {
-					/*
-					 * Open code uclamp_rq_util_with() except for
-					 * the clamp() part. Ie: apply max aggregation
-					 * only. util_fits_cpu() logic requires to
-					 * operate on non clamped util but must use the
-					 * max-aggregated uclamp_{min, max}.
-					 */
-					rq_util_min = uclamp_rq_get(cpu_rq(cpu), UCLAMP_MIN);
-					rq_util_max = uclamp_rq_get(cpu_rq(cpu), UCLAMP_MAX);
-
-					util_min = max(rq_util_min, p_util_min);
-					util_max = max(rq_util_max, p_util_max);
-				}
-			}
-			if (!util_fits_cpu(util, util_min, util_max, cpu))
+			util = uclamp_rq_util_with(cpu_rq(cpu), util, p);
+			if (!fits_capacity(util, cpu_cap))
 				continue;
 
 			lsub_positive(&cpu_cap, util);
 
 			if (cpu == prev_cpu) {
 				/* Always use prev_cpu as a candidate. */
-				prev_spare_cap = cpu_cap;
+				compute_prev_delta = true;
 			} else if (cpu_cap > max_spare_cap) {
 				/*
 				 * Find the CPU with the maximum spare capacity
-				 * among the remaining CPUs in the performance
-				 * domain.
+				 * in the performance domain.
 				 */
 				max_spare_cap = cpu_cap;
 				max_spare_cap_cpu = cpu;
 			}
 		}
 
-		if (max_spare_cap_cpu < 0 && prev_spare_cap == 0)
+		if (max_spare_cap_cpu < 0 && !compute_prev_delta)
 			continue;
 
 		eenv_pd_busy_time(&eenv, cpus, p);
@@ -7453,7 +7289,7 @@ static int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu)
 		base_energy = compute_energy(&eenv, pd, cpus, p, -1);
 
 		/* Evaluate the energy impact of using prev_cpu. */
-		if (prev_spare_cap > 0) {
+		if (compute_prev_delta) {
 			prev_delta = compute_energy(&eenv, pd, cpus, p,
 						    prev_cpu);
 			/* CPU utilization has changed */
@@ -7464,7 +7300,7 @@ static int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu)
 		}
 
 		/* Evaluate the energy impact of using max_spare_cap_cpu. */
-		if (max_spare_cap_cpu >= 0 && max_spare_cap > prev_spare_cap) {
+		if (max_spare_cap_cpu >= 0) {
 			cur_delta = compute_energy(&eenv, pd, cpus, p,
 						   max_spare_cap_cpu);
 			/* CPU utilization has changed */
@@ -8641,7 +8477,7 @@ static int detach_tasks(struct lb_env *env)
 
 		case migrate_misfit:
 			/* This is not a misfit task */
-			if (task_fits_cpu(p, env->src_cpu))
+			if (task_fits_capacity(p, capacity_of(env->src_cpu)))
 				goto next;
 
 			env->imbalance = 0;
@@ -9030,73 +8866,16 @@ static unsigned long scale_rt_capacity(int cpu)
 
 static void update_cpu_capacity(struct sched_domain *sd, int cpu)
 {
-	unsigned long capacity_orig = arch_scale_cpu_capacity(cpu);
 	unsigned long capacity = scale_rt_capacity(cpu);
 	struct sched_group *sdg = sd->groups;
-	struct rq *rq = cpu_rq(cpu);
 
-	rq->cpu_capacity_orig = capacity_orig;
+	cpu_rq(cpu)->cpu_capacity_orig = arch_scale_cpu_capacity(cpu);
 
 	if (!capacity)
 		capacity = 1;
 
-	rq->cpu_capacity = capacity;
-
-	/*
-	 * Detect if the performance domain is in capacity inversion state.
-	 *
-	 * Capacity inversion happens when another perf domain with equal or
-	 * lower capacity_orig_of() ends up having higher capacity than this
-	 * domain after subtracting thermal pressure.
-	 *
-	 * We only take into account thermal pressure in this detection as it's
-	 * the only metric that actually results in *real* reduction of
-	 * capacity due to performance points (OPPs) being dropped/become
-	 * unreachable due to thermal throttling.
-	 *
-	 * We assume:
-	 *   * That all cpus in a perf domain have the same capacity_orig
-	 *     (same uArch).
-	 *   * Thermal pressure will impact all cpus in this perf domain
-	 *     equally.
-	 */
-	if (static_branch_unlikely(&sched_asym_cpucapacity)) {
-		unsigned long inv_cap = capacity_orig - thermal_load_avg(rq);
-		struct perf_domain *pd = rcu_dereference(rq->rd->pd);
-
-		rq->cpu_capacity_inverted = 0;
-
-		for (; pd; pd = pd->next) {
-			struct cpumask *pd_span = perf_domain_span(pd);
-			unsigned long pd_cap_orig, pd_cap;
-
-			cpu = cpumask_any(pd_span);
-			pd_cap_orig = arch_scale_cpu_capacity(cpu);
-
-			if (capacity_orig < pd_cap_orig)
-				continue;
-
-			/*
-			 * handle the case of multiple perf domains have the
-			 * same capacity_orig but one of them is under higher
-			 * thermal pressure. We record it as capacity
-			 * inversion.
-			 */
-			if (capacity_orig == pd_cap_orig) {
-				pd_cap = pd_cap_orig - thermal_load_avg(cpu_rq(cpu));
-
-				if (pd_cap > inv_cap) {
-					rq->cpu_capacity_inverted = inv_cap;
-					break;
-				}
-			} else if (pd_cap_orig > inv_cap) {
-				rq->cpu_capacity_inverted = inv_cap;
-				break;
-			}
-		}
-	}
-
-	trace_sched_cpu_capacity_tp(rq);
+	cpu_rq(cpu)->cpu_capacity = capacity;
+	trace_sched_cpu_capacity_tp(cpu_rq(cpu));
 
 	sdg->sgc->capacity = capacity;
 	sdg->sgc->min_capacity = capacity;
@@ -9703,10 +9482,6 @@ static inline void update_sg_wakeup_stats(struct sched_domain *sd,
 
 	memset(sgs, 0, sizeof(*sgs));
 
-	/* Assume that task can't fit any CPU of the group */
-	if (sd->flags & SD_ASYM_CPUCAPACITY)
-		sgs->group_misfit_task_load = 1;
-
 	for_each_cpu(i, sched_group_span(group)) {
 		struct rq *rq = cpu_rq(i);
 		unsigned int local;
@@ -9726,12 +9501,12 @@ static inline void update_sg_wakeup_stats(struct sched_domain *sd,
 		if (!nr_running && idle_cpu_without(i, p))
 			sgs->idle_cpus++;
 
-		/* Check if task fits in the CPU */
-		if (sd->flags & SD_ASYM_CPUCAPACITY &&
-		    sgs->group_misfit_task_load &&
-		    task_fits_cpu(p, i))
-			sgs->group_misfit_task_load = 0;
+	}
 
+	/* Check if task fits in the group */
+	if (sd->flags & SD_ASYM_CPUCAPACITY &&
+	    !task_fits_capacity(p, group->sgc->max_capacity)) {
+		sgs->group_misfit_task_load = 1;
 	}
 
 	sgs->group_capacity = group->sgc->capacity;
diff --git a/kernel/sched/psi.c b/kernel/sched/psi.c
index dbaeac915895d..ee2ecc081422e 100644
--- a/kernel/sched/psi.c
+++ b/kernel/sched/psi.c
@@ -242,8 +242,6 @@ static void get_recent_times(struct psi_group *group, int cpu,
 			     u32 *pchanged_states)
 {
 	struct psi_group_cpu *groupc = per_cpu_ptr(group->pcpu, cpu);
-	int current_cpu = raw_smp_processor_id();
-	bool only_avgs_work = false;
 	u64 now, state_start;
 	enum psi_states s;
 	unsigned int seq;
@@ -258,15 +256,6 @@ static void get_recent_times(struct psi_group *group, int cpu,
 		memcpy(times, groupc->times, sizeof(groupc->times));
 		state_mask = groupc->state_mask;
 		state_start = groupc->state_start;
-		/*
-		 * This CPU has only avgs_work kworker running, snapshot the
-		 * newest times then don't need to re-arm for this groupc.
-		 * Normally this kworker will sleep soon and won't wake
-		 * avgs_work back up in psi_group_change().
-		 */
-		if (current_cpu == cpu && groupc->tasks[NR_RUNNING] == 1 &&
-		    !groupc->tasks[NR_IOWAIT] && !groupc->tasks[NR_MEMSTALL])
-			only_avgs_work = true;
 	} while (read_seqcount_retry(&groupc->seq, seq));
 
 	/* Calculate state time deltas against the previous snapshot */
@@ -291,10 +280,6 @@ static void get_recent_times(struct psi_group *group, int cpu,
 		if (delta)
 			*pchanged_states |= (1 << s);
 	}
-
-	/* Clear PSI_NONIDLE so avgs_work won't be re-armed for this groupc */
-	if (only_avgs_work)
-		*pchanged_states &= ~(1 << PSI_NONIDLE);
 }
 
 static void calc_avgs(unsigned long avg[3], int missed_periods,
@@ -554,12 +539,10 @@ static u64 update_triggers(struct psi_group *group, u64 now)
 
 			/* Calculate growth since last update */
 			growth = window_update(&t->win, now, total[t->state]);
-			if (!t->pending_event) {
-				if (growth < t->threshold)
-					continue;
+			if (growth < t->threshold)
+				continue;
 
-				t->pending_event = true;
-			}
+			t->pending_event = true;
 		}
 		/* Limit event signaling to once per window */
 		if (now < t->last_event_time + t->win.size)
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2e9ae4d54c202..17b846e719efe 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1076,7 +1076,6 @@ struct rq {
 
 	unsigned long		cpu_capacity;
 	unsigned long		cpu_capacity_orig;
-	unsigned long		cpu_capacity_inverted;
 
 	struct balance_callback *balance_callback;
 
@@ -1186,9 +1185,6 @@ struct rq {
 	unsigned int		core_forceidle_occupation;
 	u64			core_forceidle_start;
 #endif
-
-	/* Scratch cpumask to be temporarily used under rq_lock */
-	cpumask_var_t		scratch_mask;
 };
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
@@ -1916,13 +1912,6 @@ static inline void dirty_sched_domain_sysctl(int cpu)
 #endif
 
 extern int sched_update_scaling(void);
-
-static inline const struct cpumask *task_user_cpus(struct task_struct *p)
-{
-	if (!p->user_cpus_ptr)
-		return cpu_possible_mask; /* &init_task.cpus_mask */
-	return p->user_cpus_ptr;
-}
 #endif /* CONFIG_SMP */
 
 #include "stats.h"
@@ -2191,12 +2180,6 @@ extern const int		sched_latency_to_weight[40];
 
 #define RETRY_TASK		((void *)-1UL)
 
-struct affinity_context {
-	const struct cpumask *new_mask;
-	struct cpumask *user_mask;
-	unsigned int flags;
-};
-
 struct sched_class {
 
 #ifdef CONFIG_UCLAMP_TASK
@@ -2225,7 +2208,9 @@ struct sched_class {
 
 	void (*task_woken)(struct rq *this_rq, struct task_struct *task);
 
-	void (*set_cpus_allowed)(struct task_struct *p, struct affinity_context *ctx);
+	void (*set_cpus_allowed)(struct task_struct *p,
+				 const struct cpumask *newmask,
+				 u32 flags);
 
 	void (*rq_online)(struct rq *rq);
 	void (*rq_offline)(struct rq *rq);
@@ -2336,7 +2321,7 @@ extern void update_group_capacity(struct sched_domain *sd, int cpu);
 
 extern void trigger_load_balance(struct rq *rq);
 
-extern void set_cpus_allowed_common(struct task_struct *p, struct affinity_context *ctx);
+extern void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask, u32 flags);
 
 static inline struct task_struct *get_push_task(struct rq *rq)
 {
@@ -2957,24 +2942,6 @@ static inline unsigned long capacity_orig_of(int cpu)
 	return cpu_rq(cpu)->cpu_capacity_orig;
 }
 
-/*
- * Returns inverted capacity if the CPU is in capacity inversion state.
- * 0 otherwise.
- *
- * Capacity inversion detection only considers thermal impact where actual
- * performance points (OPPs) gets dropped.
- *
- * Capacity inversion state happens when another performance domain that has
- * equal or lower capacity_orig_of() becomes effectively larger than the perf
- * domain this CPU belongs to due to thermal pressure throttling it hard.
- *
- * See comment in update_cpu_capacity().
- */
-static inline unsigned long cpu_in_capacity_inversion(int cpu)
-{
-	return cpu_rq(cpu)->cpu_capacity_inverted;
-}
-
 /**
  * enum cpu_util_type - CPU utilization type
  * @FREQUENCY_UTIL:	Utilization used to select frequency
@@ -3076,23 +3043,6 @@ static inline unsigned long cpu_util_rt(struct rq *rq)
 #ifdef CONFIG_UCLAMP_TASK
 unsigned long uclamp_eff_value(struct task_struct *p, enum uclamp_id clamp_id);
 
-static inline unsigned long uclamp_rq_get(struct rq *rq,
-					  enum uclamp_id clamp_id)
-{
-	return READ_ONCE(rq->uclamp[clamp_id].value);
-}
-
-static inline void uclamp_rq_set(struct rq *rq, enum uclamp_id clamp_id,
-				 unsigned int value)
-{
-	WRITE_ONCE(rq->uclamp[clamp_id].value, value);
-}
-
-static inline bool uclamp_rq_is_idle(struct rq *rq)
-{
-	return rq->uclamp_flags & UCLAMP_FLAG_IDLE;
-}
-
 /**
  * uclamp_rq_util_with - clamp @util with @rq and @p effective uclamp values.
  * @rq:		The rq to clamp against. Must not be NULL.
@@ -3128,12 +3078,12 @@ unsigned long uclamp_rq_util_with(struct rq *rq, unsigned long util,
 		 * Ignore last runnable task's max clamp, as this task will
 		 * reset it. Similarly, no need to read the rq's min clamp.
 		 */
-		if (uclamp_rq_is_idle(rq))
+		if (rq->uclamp_flags & UCLAMP_FLAG_IDLE)
 			goto out;
 	}
 
-	min_util = max_t(unsigned long, min_util, uclamp_rq_get(rq, UCLAMP_MIN));
-	max_util = max_t(unsigned long, max_util, uclamp_rq_get(rq, UCLAMP_MAX));
+	min_util = max_t(unsigned long, min_util, READ_ONCE(rq->uclamp[UCLAMP_MIN].value));
+	max_util = max_t(unsigned long, max_util, READ_ONCE(rq->uclamp[UCLAMP_MAX].value));
 out:
 	/*
 	 * Since CPU's {min,max}_util clamps are MAX aggregated considering
@@ -3174,15 +3124,6 @@ static inline bool uclamp_is_used(void)
 	return static_branch_likely(&sched_uclamp_used);
 }
 #else /* CONFIG_UCLAMP_TASK */
-static inline unsigned long uclamp_eff_value(struct task_struct *p,
-					     enum uclamp_id clamp_id)
-{
-	if (clamp_id == UCLAMP_MIN)
-		return 0;
-
-	return SCHED_CAPACITY_SCALE;
-}
-
 static inline
 unsigned long uclamp_rq_util_with(struct rq *rq, unsigned long util,
 				  struct task_struct *p)
@@ -3196,25 +3137,6 @@ static inline bool uclamp_is_used(void)
 {
 	return false;
 }
-
-static inline unsigned long uclamp_rq_get(struct rq *rq,
-					  enum uclamp_id clamp_id)
-{
-	if (clamp_id == UCLAMP_MIN)
-		return 0;
-
-	return SCHED_CAPACITY_SCALE;
-}
-
-static inline void uclamp_rq_set(struct rq *rq, enum uclamp_id clamp_id,
-				 unsigned int value)
-{
-}
-
-static inline bool uclamp_rq_is_idle(struct rq *rq)
-{
-	return false;
-}
 #endif /* CONFIG_UCLAMP_TASK */
 
 #ifdef CONFIG_HAVE_SCHED_AVG_IRQ
-- 
2.38.1.381.gc03801e19c


From a6ea70c35e5d2d56f8219313e6e8d4b180ab290d Mon Sep 17 00:00:00 2001
From: Peter Jung <admin@ptr1337.dev>
Date: Mon, 31 Oct 2022 18:16:16 +0100
Subject: [PATCH] NEST-global-cachy

Signed-off-by: Peter Jung <admin@ptr1337.dev>
---
 arch/x86/kernel/cpu/aperfmperf.c |   2 +
 include/linux/sched.h            |  14 ++
 include/linux/sched/sysctl.h     |   2 +
 include/linux/sched/topology.h   |   2 +
 kernel/sched/core.c              |  87 ++++++++-
 kernel/sched/fair.c              | 301 ++++++++++++++++++++++++++++++-
 kernel/sched/idle.c              |  20 ++
 kernel/sched/sched.h             |  49 +++++
 kernel/sysctl.c                  |   9 +
 9 files changed, 471 insertions(+), 15 deletions(-)

diff --git a/arch/x86/kernel/cpu/aperfmperf.c b/arch/x86/kernel/cpu/aperfmperf.c
index 1f60a2b279368..3c992adbe3cf3 100644
--- a/arch/x86/kernel/cpu/aperfmperf.c
+++ b/arch/x86/kernel/cpu/aperfmperf.c
@@ -383,6 +383,8 @@ void arch_scale_freq_tick(void)
 	acnt = aperf - s->aperf;
 	mcnt = mperf - s->mperf;
 
+	trace_printk("freq %lld\n", div64_u64((cpu_khz * acnt), mcnt));
+
 	s->aperf = aperf;
 	s->mperf = mperf;
 
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2d2a7ded02644..a220e29ffaf24 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -742,6 +742,17 @@ struct kmap_ctrl {
 #endif
 };
 
+struct expand_mask {
+	spinlock_t			lock;
+	cpumask_t			expand_mask, reserve_mask;
+	int				start;
+	int				count;
+};
+
+extern void init_expand_mask(void);
+extern void clear_expand_mask(int cpu);
+extern void reset_expand_mask(int cpu);
+
 struct task_struct {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/*
@@ -785,6 +796,9 @@ struct task_struct {
 	 */
 	int				recent_used_cpu;
 	int				wake_cpu;
+	int				use_expand_mask;
+	int				patience;
+	int				attached;
 #endif
 	int				on_rq;
 
diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 303ee7dd0c7e2..ee9c03e089540 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -21,6 +21,8 @@ enum sched_tunable_scaling {
 	SCHED_TUNABLESCALING_END,
 };
 
+extern unsigned int sysctl_sched_nest;
+
 #define NUMA_BALANCING_DISABLED		0x0
 #define NUMA_BALANCING_NORMAL		0x1
 #define NUMA_BALANCING_MEMORY_TIERING	0x2
diff --git a/include/linux/sched/topology.h b/include/linux/sched/topology.h
index 816df6cc444e1..54e577ef83bb8 100644
--- a/include/linux/sched/topology.h
+++ b/include/linux/sched/topology.h
@@ -82,6 +82,8 @@ struct sched_domain_shared {
 	atomic_t	nr_busy_cpus;
 	int		has_idle_cores;
 	int		nr_idle_scan;
+	int		has_idle_threads;
+	int		left_off;
 };
 
 struct sched_domain {
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 2724dcaabe8e7..bca39ad9ef6c4 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -2592,6 +2592,7 @@ __do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask, u32
 
 void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
 {
+	p->use_expand_mask = 0;
 	__do_set_cpus_allowed(p, new_mask, 0);
 }
 
@@ -3505,8 +3506,11 @@ int select_task_rq(struct task_struct *p, int cpu, int wake_flags)
 	 * [ this allows ->select_task() to simply return task_cpu(p) and
 	 *   not worry about this generic constraint ]
 	 */
-	if (unlikely(!is_cpu_allowed(p, cpu)))
+	if (unlikely(!is_cpu_allowed(p, cpu))) {
+		if (sysctl_sched_nest)
+			atomic_set(&cpu_rq(p->thread_info.cpu)->taken,0);
 		cpu = select_fallback_rq(task_cpu(p), p);
+	}
 
 	return cpu;
 }
@@ -3732,13 +3736,6 @@ void sched_ttwu_pending(void *arg)
 	if (!llist)
 		return;
 
-	/*
-	 * rq::ttwu_pending racy indication of out-standing wakeups.
-	 * Races such that false-negatives are possible, since they
-	 * are shorter lived that false-positives would be.
-	 */
-	WRITE_ONCE(rq->ttwu_pending, 0);
-
 	rq_lock_irqsave(rq, &rf);
 	update_rq_clock(rq);
 
@@ -3753,6 +3750,13 @@ void sched_ttwu_pending(void *arg)
 	}
 
 	rq_unlock_irqrestore(rq, &rf);
+
+	/*
+	 * rq::ttwu_pending racy indication of out-standing wakeups.
+	 * Races such that false-negatives are possible, since they
+	 * are shorter lived that false-positives would be.
+	 */
+	WRITE_ONCE(rq->ttwu_pending, 0);
 }
 
 void send_call_function_single_ipi(int cpu)
@@ -4203,6 +4207,8 @@ try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
 #endif /* CONFIG_SMP */
 
 	ttwu_queue(p, cpu, wake_flags);
+	if (sysctl_sched_nest)
+		atomic_set(&cpu_rq(cpu)->taken,0);
 unlock:
 	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
 out:
@@ -4680,7 +4686,7 @@ unsigned long to_ratio(u64 period, u64 runtime)
 void wake_up_new_task(struct task_struct *p)
 {
 	struct rq_flags rf;
-	struct rq *rq;
+	struct rq *rq = cpu_rq(smp_processor_id());
 
 	raw_spin_lock_irqsave(&p->pi_lock, rf.flags);
 	WRITE_ONCE(p->__state, TASK_RUNNING);
@@ -4695,12 +4701,16 @@ void wake_up_new_task(struct task_struct *p)
 	 */
 	p->recent_used_cpu = task_cpu(p);
 	rseq_migrate(p);
+	p->attached = -1;
+	p->patience = INIT_PATIENCE/*cpumask_weight(&p->expand_cpus_mask->mask)*/ + 1;
 	__set_task_cpu(p, select_task_rq(p, task_cpu(p), WF_FORK));
 #endif
 	rq = __task_rq_lock(p, &rf);
 	update_rq_clock(rq);
 	post_init_entity_util_avg(p);
 
+	if (sysctl_sched_nest)
+		atomic_set(&cpu_rq(p->thread_info.cpu)->taken,0);
 	activate_task(rq, p, ENQUEUE_NOCLOCK);
 	trace_sched_wakeup_new(p);
 	check_preempt_curr(rq, p, WF_FORK);
@@ -5011,6 +5021,22 @@ static inline void
 prepare_task_switch(struct rq *rq, struct task_struct *prev,
 		    struct task_struct *next)
 {
+	if (sysctl_sched_nest) {
+		if (next->pid == 0) {
+			smp_mb__before_atomic();
+			start_spinning(rq->cpu);
+			atomic_set(&rq->drop_expand_ctr, EXPAND_DELAY); // drop_expand is 0
+			atomic_set(&rq->drop_reserve_ctr, RESERVE_DELAY); // drop_reserve is 0
+			smp_mb__after_atomic();
+		} else {
+			smp_mb__before_atomic();
+			atomic_set(&rq->drop_expand_ctr,0);
+			atomic_set(&rq->drop_reserve_ctr,0);
+			smp_mb__after_atomic();
+			rq->drop_expand = 0;
+			rq->drop_reserve = 0;
+		}
+	}
 	kcov_prepare_switch(prev);
 	sched_info_switch(rq, prev, next);
 	perf_event_task_sched_out(prev, next);
@@ -5489,6 +5515,15 @@ void scheduler_tick(void)
 	perf_event_task_tick();
 
 #ifdef CONFIG_SMP
+	smp_mb__before_atomic();
+	atomic_dec_if_positive(&rq->should_spin);
+	if (atomic_fetch_add_unless(&rq->drop_expand_ctr,-1,0) == 1) {
+		trace_printk("counter %d, so changing %d to 1\n", atomic_read(&rq->drop_expand_ctr), rq->drop_expand);
+		rq->drop_expand = 1;
+	}
+	if (atomic_fetch_add_unless(&rq->drop_reserve_ctr,-1,0) == 1)
+		rq->drop_reserve = 1;
+	smp_mb__after_atomic();
 	rq->idle_balance = idle_cpu(cpu);
 	trigger_load_balance(rq);
 #endif
@@ -6515,6 +6550,17 @@ static void __sched notrace __schedule(unsigned int sched_mode)
 
 		trace_sched_switch(sched_mode & SM_MASK_PREEMPT, prev, next, prev_state);
 
+		if (sysctl_sched_nest && prev_state & TASK_DEAD && available_idle_cpu(cpu)) {
+			smp_mb__before_atomic();
+			atomic_set(&rq->should_spin,0); // clean up early
+			atomic_set(&rq->drop_expand_ctr,0);
+			atomic_set(&rq->drop_reserve_ctr,0);
+			smp_mb__after_atomic();
+			rq->drop_expand = 0; // prepare for next time
+			rq->drop_reserve = 0; // prepare for next time
+			trace_printk("%d terminated so clear core %d\n",prev->pid,cpu);
+			clear_expand_mask(cpu);
+		}
 		/* Also unlocks the rq: */
 		rq = context_switch(rq, prev, next, &rf);
 	} else {
@@ -8236,14 +8282,30 @@ SYSCALL_DEFINE3(sched_setaffinity, pid_t, pid, unsigned int, len,
 		unsigned long __user *, user_mask_ptr)
 {
 	cpumask_var_t new_mask;
+	cpumask_t tmp;
+	struct task_struct *p;
 	int retval;
 
+	rcu_read_lock();
+	p = find_process_by_pid(pid);
+	if (!p) {
+		rcu_read_unlock();
+		return -ESRCH;
+	}
+	rcu_read_unlock();
 	if (!alloc_cpumask_var(&new_mask, GFP_KERNEL))
 		return -ENOMEM;
 
 	retval = get_user_cpu_mask(user_mask_ptr, len, new_mask);
 	if (retval == 0)
 		retval = sched_setaffinity(pid, new_mask);
+
+	cpumask_xor(&tmp,new_mask,cpu_possible_mask);
+	if (cpumask_weight(&tmp) == 0) {
+		int cpu = task_cpu(p);
+		reset_expand_mask(cpu);
+		p->use_expand_mask = 1;
+	}
 	free_cpumask_var(new_mask);
 	return retval;
 }
@@ -9720,6 +9782,7 @@ void __init sched_init(void)
 	autogroup_init(&init_task);
 #endif /* CONFIG_CGROUP_SCHED */
 
+	init_expand_mask();
 	for_each_possible_cpu(i) {
 		struct rq *rq;
 
@@ -9776,6 +9839,12 @@ void __init sched_init(void)
 		rq->wake_avg_idle = rq->avg_idle;
 		rq->max_idle_balance_cost = sysctl_sched_migration_cost;
 
+		atomic_set(&rq->should_spin,0);
+		atomic_set(&rq->taken,0);
+		atomic_set(&rq->drop_expand_ctr,0);
+		atomic_set(&rq->drop_reserve_ctr,0);
+		rq->drop_expand = 0;
+		rq->drop_reserve = 0;
 		INIT_LIST_HEAD(&rq->cfs_tasks);
 
 		rq_attach_root(rq, &def_root_domain);
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 3574b93e71af8..b090c10697d82 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -150,6 +150,8 @@ unsigned int __read_mostly sched_burst_granularity   = 5;
 unsigned int __read_mostly sched_burst_reduction     = 3;
 #endif // CONFIG_SCHED_BORE
 
+unsigned int __read_mostly sysctl_sched_nest = 0;
+
 int sched_thermal_decay_shift;
 static int __init setup_sched_thermal_decay_shift(char *str)
 {
@@ -3743,6 +3745,61 @@ static inline void update_tg_load_avg(struct cfs_rq *cfs_rq)
  * caller only guarantees p->pi_lock is held; no other assumptions,
  * including the state of rq->lock, should be made.
  */
+
+static struct expand_mask expand_mask;
+
+void init_expand_mask(void) {
+	spin_lock_init(&expand_mask.lock);
+}
+
+static inline void set_expand_mask(int cpu) {
+	spin_lock(&expand_mask.lock);
+	cpumask_set_cpu(cpu, &expand_mask.expand_mask);
+	if (cpumask_test_cpu(cpu,&expand_mask.reserve_mask)) {
+		cpumask_clear_cpu(cpu, &expand_mask.reserve_mask);
+		expand_mask.count--;
+	}
+	spin_unlock(&expand_mask.lock);
+}
+
+void clear_expand_mask(int cpu) { // cpu is set in expand mask
+	spin_lock(&expand_mask.lock);
+	cpumask_clear_cpu(cpu, &expand_mask.expand_mask);
+	if (!cpumask_test_cpu(cpu,&expand_mask.reserve_mask) && expand_mask.count < RESERVE_MAX) {
+		cpumask_set_cpu(cpu, &expand_mask.reserve_mask);
+		expand_mask.count++;
+	}
+	spin_unlock(&expand_mask.lock);
+}
+
+static inline void set_reserve_mask(int cpu) {
+	spin_lock(&expand_mask.lock);
+	if (!cpumask_test_cpu(cpu,&expand_mask.expand_mask) && !cpumask_test_cpu(cpu,&expand_mask.reserve_mask) && expand_mask.count < RESERVE_MAX) {
+		cpumask_set_cpu(cpu, &expand_mask.reserve_mask);
+		expand_mask.count++;
+	}
+	spin_unlock(&expand_mask.lock);
+}
+
+static inline void clear_reserve_mask(int cpu) { // cpu is set in reserve mask
+	spin_lock(&expand_mask.lock);
+	if (expand_mask.count > RESERVE_MAX) {
+		cpumask_clear_cpu(cpu, &expand_mask.reserve_mask);
+		expand_mask.count--;
+	}
+	spin_unlock(&expand_mask.lock);
+}
+
+void reset_expand_mask(int cpu) {
+	spin_lock(&expand_mask.lock);
+	cpumask_clear(&expand_mask.expand_mask);
+	cpumask_set_cpu(cpu, &expand_mask.expand_mask);
+	cpumask_clear(&expand_mask.reserve_mask);
+	expand_mask.start = cpu;
+	expand_mask.count = 0;
+	spin_unlock(&expand_mask.lock);
+}
+
 void set_task_rq_fair(struct sched_entity *se,
 		      struct cfs_rq *prev, struct cfs_rq *next)
 {
@@ -6601,6 +6658,46 @@ static inline bool test_idle_cores(int cpu)
 	return false;
 }
 
+static inline void set_idle_threads(int cpu, int val)
+{
+	struct sched_domain_shared *sds;
+
+	sds = rcu_dereference(per_cpu(sd_llc_shared, cpu));
+	if (sds)
+		WRITE_ONCE(sds->has_idle_threads, val);
+}
+
+static inline bool test_idle_threads(int cpu, bool def)
+{
+	struct sched_domain_shared *sds;
+
+	sds = rcu_dereference(per_cpu(sd_llc_shared, cpu));
+	if (sds)
+		return READ_ONCE(sds->has_idle_threads);
+
+	return def;
+}
+
+static inline void set_left_off(int cpu, int val)
+{
+	struct sched_domain_shared *sds;
+
+	sds = rcu_dereference(per_cpu(sd_llc_shared, cpu));
+	if (sds)
+		WRITE_ONCE(sds->left_off, val);
+}
+
+static inline int get_left_off(int cpu, int def)
+{
+	struct sched_domain_shared *sds;
+
+	sds = rcu_dereference(per_cpu(sd_llc_shared, cpu));
+	if (sds)
+		return READ_ONCE(sds->left_off);
+
+	return def;
+}
+
 /*
  * Scans the local SMT mask to see if the entire core is idle, and records this
  * information in sd_llc_shared->has_idle_cores.
@@ -6617,6 +6714,9 @@ void __update_idle_core(struct rq *rq)
 	if (test_idle_cores(core))
 		goto unlock;
 
+	if (!test_idle_cores(core, true))
+		set_idle_threads(core, 1);
+
 	for_each_cpu(cpu, cpu_smt_mask(core)) {
 		if (cpu == core)
 			continue;
@@ -6711,7 +6811,7 @@ static inline int select_idle_smt(struct task_struct *p, int target)
 static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool has_idle_core, int target)
 {
 	struct cpumask *cpus = this_cpu_cpumask_var_ptr(select_rq_mask);
-	int i, cpu, idle_cpu = -1, nr = INT_MAX;
+	int i, cpu, idle_cpu = -1, nr = INT_MAX, newtarget;
 	struct sched_domain_shared *sd_share;
 	struct rq *this_rq = this_rq();
 	int this = smp_processor_id();
@@ -6763,23 +6863,29 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool
 		}
 	}
 
-	for_each_cpu_wrap(cpu, cpus, target + 1) {
+	newtarget = get_left_off(this, target);
+	for_each_cpu_wrap(cpu, cpus, newtarget) {
 		if (has_idle_core) {
 			i = select_idle_core(p, cpu, cpus, &idle_cpu);
 			if ((unsigned int)i < nr_cpumask_bits)
 				return i;
 
 		} else {
-			if (!--nr)
+			if (!--nr) {
+				set_left_off(this, cpu);
 				return -1;
+			}
 			idle_cpu = __select_idle_cpu(cpu, p);
 			if ((unsigned int)idle_cpu < nr_cpumask_bits)
 				break;
 		}
 	}
 
-	if (has_idle_core)
+	if (has_idle_core) {
 		set_idle_cores(target, false);
+		if (idle_cpu == -1)
+			set_idle_threads(target, false);
+	}
 
 	if (sched_feat(SIS_PROP) && this_sd && !has_idle_core) {
 		time = cpu_clock(this) - time;
@@ -6844,9 +6950,11 @@ static inline bool asym_fits_capacity(unsigned long task_util, int cpu)
 static int select_idle_sibling(struct task_struct *p, int prev, int target)
 {
 	bool has_idle_core = false;
-	struct sched_domain *sd;
+	struct sched_domain *sd, *sd1;
+	struct sched_group *group, *group0;
 	unsigned long task_util;
 	int i, recent_used_cpu;
+	int otarget = target;
 
 	/*
 	 * On asymmetric system, update task utilization because we will check
@@ -6940,7 +7048,41 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	if ((unsigned)i < nr_cpumask_bits)
 		return i;
 
-	return target;
+	if (!sd->parent) goto abort;
+	group0 = group = sd->parent->groups;
+	do {
+		struct cpumask *m = sched_group_span(group);
+
+		if (cpumask_test_cpu(otarget, m))
+			goto next;
+
+		sd1 = rcu_dereference(per_cpu(sd_llc, cpumask_first(sched_group_span(group))));
+		if (!sd1 || !READ_ONCE(sd1->shared->has_idle_threads))
+			goto next;
+
+		target = cpumask_next_wrap(otarget,m,otarget,true);
+
+		if (sched_smt_active()) {
+			has_idle_core = test_idle_cores(target, false);
+
+			if (!has_idle_core && cpus_share_cache(prev, target)) {
+				i = select_idle_smt(p, sd1, prev);
+				if ((unsigned int)i < nr_cpumask_bits)
+					return i;
+			}
+		}
+
+		i = select_idle_cpu(p, sd1, has_idle_core, target);
+		if ((unsigned)i < nr_cpumask_bits)
+			return i;
+
+next:
+		group = group->next;
+	} while (group != group0);
+
+
+abort:
+	return otarget;
 }
 
 /*
@@ -7344,6 +7486,7 @@ select_task_rq_fair(struct task_struct *p, int prev_cpu, int wake_flags)
 	int cpu = smp_processor_id();
 	int new_cpu = prev_cpu;
 	int want_affine = 0;
+	int impatient = 0;
 	/* SD_flags and WF_flags share the first nibble */
 	int sd_flag = wake_flags & 0xF;
 
@@ -7365,6 +7508,144 @@ select_task_rq_fair(struct task_struct *p, int prev_cpu, int wake_flags)
 	}
 
 	rcu_read_lock();
+
+	if (sysctl_sched_nest) {
+		struct sched_domain *this_sd;
+		cpumask_t *mask_sd;
+		int i;
+		if (sd_flag & SD_BALANCE_EXEC) {
+			new_cpu = prev_cpu; // why move???
+			goto out;
+		}
+
+		if (p->attached >= 0) {
+			if (cpumask_test_cpu(p->attached,&expand_mask.expand_mask) && cpumask_test_cpu(p->attached,p->cpus_ptr) && available_idle_cpu(p->attached) && !atomic_cmpxchg(&cpu_rq(p->attached)->taken,0,1)) {
+				new_cpu = p->attached;
+				//trace_printk("attached\n");
+				if (new_cpu == prev_cpu)
+					p->patience = INIT_PATIENCE;
+				goto out;
+			}
+			if (p->attached != prev_cpu || !cpumask_test_cpu(p->attached,&expand_mask.expand_mask))
+				p->attached = -1;
+		}
+		// if the thread is not attached somewhere and it is placed outside the mask, then this is a good core for the thread and the core should be in the mask
+		else
+		if (!cpumask_test_cpu(prev_cpu,&expand_mask.expand_mask) && cpumask_test_cpu(prev_cpu,p->cpus_ptr) && available_idle_cpu(prev_cpu) && !atomic_cmpxchg(&cpu_rq(prev_cpu)->taken,0,1)) {
+			p->attached = new_cpu = prev_cpu;
+			p->patience = INIT_PATIENCE;
+			set_expand_mask(prev_cpu);
+			goto out;
+		}
+		this_sd = rcu_dereference(*per_cpu_ptr(&sd_llc,prev_cpu));
+		mask_sd = sched_domain_span(this_sd);
+		for_each_cpu_wrap(i, &expand_mask.expand_mask, prev_cpu) {
+			if (
+				cpumask_test_cpu(i,mask_sd) &&
+				cpumask_test_cpu(i,p->cpus_ptr) && available_idle_cpu(i)) {
+				struct rq *rq = cpu_rq(i);
+				if (rq->drop_expand) {
+					//trace_printk("dropping %d from expand mask (same socket)\n",i);
+					clear_expand_mask(i);
+					rq->drop_expand = 0;
+					smp_mb__before_atomic();
+					atomic_set(&rq->drop_expand_ctr,0);
+					smp_mb__after_atomic();
+					continue;
+				}
+				if (!atomic_cmpxchg(&cpu_rq(i)->taken,0,1)) {
+					new_cpu = i;
+					//trace_printk("in mask same socket\n");
+					if (new_cpu == prev_cpu) {
+						p->attached = new_cpu;
+						p->patience = INIT_PATIENCE;
+					}
+					goto out;
+				}
+			}
+			if (i == prev_cpu) {
+				if (p->patience)
+					p->patience--;
+				else { // leave expand mask - too small
+					impatient = 1;
+					p->patience = INIT_PATIENCE;
+					//trace_printk("impatient\n");
+					goto reserve;
+				}
+			}
+		}
+		for_each_cpu_wrap(i, &expand_mask.expand_mask, prev_cpu) {
+			if (!cpumask_test_cpu(i,mask_sd) && cpumask_test_cpu(i,p->cpus_ptr) && available_idle_cpu(i)) {
+				struct rq *rq = cpu_rq(i);
+				if (rq->drop_expand) {
+					//trace_printk("dropping %d from expand mask (other socket)\n",i);
+					clear_expand_mask(i);
+					rq->drop_expand = 0;
+					smp_mb__before_atomic();
+					atomic_set(&rq->drop_expand_ctr,0);
+					smp_mb__after_atomic();
+					continue;
+				}
+				if (!atomic_cmpxchg(&cpu_rq(i)->taken,0,1)) {
+					new_cpu = i;
+					//trace_printk("in mask other socket\n");
+					if (new_cpu == prev_cpu) {
+						p->attached = new_cpu;
+						p->patience = INIT_PATIENCE;
+					}
+					goto out;
+				}
+			}
+		}
+reserve:
+		for_each_cpu_wrap(i, &expand_mask.reserve_mask, expand_mask.start) {
+			if (cpumask_test_cpu(i,mask_sd) && cpumask_test_cpu(i,p->cpus_ptr) && available_idle_cpu(i)) {
+				struct rq *rq = cpu_rq(i);
+				if (rq->drop_reserve) {
+					clear_reserve_mask(i);
+					rq->drop_reserve = 0;
+					smp_mb__before_atomic();
+					atomic_set(&rq->drop_reserve_ctr,0);
+					smp_mb__after_atomic();
+					continue;
+				}
+				if (!atomic_cmpxchg(&cpu_rq(i)->taken,0,1)) {
+					new_cpu = i;
+					//trace_printk("reserve same socket\n");
+					if (new_cpu == prev_cpu) {
+						p->attached = new_cpu;
+						p->patience = INIT_PATIENCE;
+					}
+					set_expand_mask(new_cpu);
+					goto out;
+				}
+			}
+		}
+		for_each_cpu_wrap(i, &expand_mask.reserve_mask, expand_mask.start) {
+			if (!cpumask_test_cpu(i,mask_sd) && cpumask_test_cpu(i,p->cpus_ptr) && available_idle_cpu(i)) {
+				struct rq *rq = cpu_rq(i);
+				if (rq->drop_reserve) {
+					clear_reserve_mask(i);
+					rq->drop_reserve = 0;
+					smp_mb__before_atomic();
+					atomic_set(&rq->drop_reserve_ctr,0);
+					smp_mb__after_atomic();
+					continue;
+				}
+				if (!atomic_cmpxchg(&cpu_rq(i)->taken,0,1)) {
+					new_cpu = i;
+					//trace_printk("reserve other socket\n");
+					if (new_cpu == prev_cpu) {
+						p->attached = new_cpu;
+						p->patience = INIT_PATIENCE;
+					}
+					set_expand_mask(new_cpu);
+					goto out;
+				}
+			}
+		}
+	}
+
 	for_each_domain(cpu, tmp) {
 		/*
 		 * If both 'cpu' and 'prev_cpu' are part of this domain,
@@ -7397,6 +7678,14 @@ select_task_rq_fair(struct task_struct *p, int prev_cpu, int wake_flags)
 		/* Fast path */
 		new_cpu = select_idle_sibling(p, prev_cpu, new_cpu);
 	}
+	if (sysctl_sched_nest) {
+		if (impatient)
+			set_expand_mask(new_cpu);
+		else
+			set_reserve_mask(new_cpu);
+	}
+
+out:
 	rcu_read_unlock();
 
 	return new_cpu;
diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index f26ab2675f7d7..ae82397190265 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -259,6 +259,26 @@ static void do_idle(void)
 {
 	int cpu = smp_processor_id();
 
+	if (atomic_read(&cpu_rq(cpu)->should_spin)) {
+		int sibling, spinning = 1;
+		const cpumask_t *m = cpu_smt_mask(cpu);
+		struct rq *rq = cpu_rq(cpu);
+		while (!tif_need_resched() && atomic_read(&rq->should_spin)) {
+			for_each_cpu(sibling, m)
+				if (sibling != cpu && cpu_rq(sibling)->nr_running) {
+					atomic_set(&rq->should_spin,0);
+					spinning = 0;
+					break;
+				}
+		}
+		if (tif_need_resched()) {
+			if (spinning)
+				atomic_set(&rq->should_spin,0);
+			schedule_idle();
+			return;
+		}
+	}
+
 	/*
 	 * Check if we need to update blocked load
 	 */
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 17b846e719efe..bdaaf2cf6e77b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1011,6 +1011,8 @@ struct rq {
 
 #ifdef CONFIG_SMP
 	unsigned int		ttwu_pending;
+	atomic_t		should_spin, drop_expand_ctr, drop_reserve_ctr, taken;
+	int			drop_expand, drop_reserve;
 #endif
 	u64			nr_switches;
 
@@ -1433,6 +1435,53 @@ static inline void update_idle_core(struct rq *rq)
 static inline void update_idle_core(struct rq *rq) { }
 #endif
 
+#ifndef NO_NEST_SPIN_DELAY
+#define SPIN_DELAY 2  // nest value
+#else
+#ifndef NO_NEST_SPIN_DELAY_T10
+#define SPIN_DELAY 4
+#else
+#define SPIN_DELAY 20
+#endif
+#endif
+#ifndef NO_NEST_EXPAND_DELAY
+#define EXPAND_DELAY 2  // nest value
+#else
+#ifndef NO_NEST_EXPAND_DELAY_T10
+#define EXPAND_DELAY 4
+#else
+#define EXPAND_DELAY 20
+#endif
+#endif
+#define RESERVE_DELAY 20
+#ifndef NO_NEST_INIT_PATIENCE
+#define INIT_PATIENCE 2  // nest value
+#else
+#ifndef NO_NEST_INIT_PATIENCE_T10
+#define INIT_PATIENCE 4
+#else
+#define INIT_PATIENCE 20
+#endif
+#endif
+#ifndef NO_NEST_RESERVE_MAX
+#define RESERVE_MAX 5
+#else
+#ifndef NO_NEST_RESERVE_MAX_T10
+#define RESERVE_MAX 10
+#else
+#define RESERVE_MAX 50
+#endif
+#endif
+
+static void inline start_spinning(int cpu) {
+	int sibling;
+
+	for_each_cpu(sibling, cpu_smt_mask(cpu))
+		if (sibling != cpu && (!available_idle_cpu(sibling) || atomic_read(&cpu_rq(sibling)->should_spin)))
+			return;
+	atomic_set(&cpu_rq(cpu)->should_spin,SPIN_DELAY);
+}
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
 static inline struct task_struct *task_of(struct sched_entity *se)
 {
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 20e98bb81f5bb..aaea480de2bb4 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -2129,6 +2129,15 @@ static struct ctl_table kern_table[] = {
 		.extra1		= SYSCTL_ZERO,
 		.extra2		= SYSCTL_ONE,
 	},
+	{
+		.procname	= "sched_nest",
+		.data		= &sysctl_sched_nest,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
+	},
 #ifdef CONFIG_TREE_RCU
 	{
 		.procname	= "panic_on_rcu_stall",
-- 
2.38.1.381.gc03801e19c

